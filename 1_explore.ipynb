{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Science\n",
    "\n",
    "Data exploration and understanding the task at hand is a fundamental step in the Machine Learning workflow.\n",
    "In this notebook, we'll take an opportunity to explore the use case, data and models we'll be using.\n",
    "\n",
    "We have been tasked with developing an application which can identify objects in static and live images. In this notebook we use a pre-trained machine learning model, and explore how it works on static photos. \n",
    "\n",
    "To begin, we import a range of python packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import environ\n",
    "\n",
    "from boto3 import client\n",
    "import cv2\n",
    "import numpy as np\n",
    "from onnxruntime import InferenceSession\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from classes import default_class_labels\n",
    "\n",
    "import random\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model ID and associated class labels are used further below. Update in case you want to work with a new model, e.g. with\n",
    "\n",
    "`class_labels = ['Laptop', 'Computer keyboard', 'Table']`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_id = 'yolov5m'\n",
    "class_labels = default_class_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import our image\n",
    "\n",
    "In the next cell we import the image we want to test our model on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_image = 'sample-images/scene.jpg'\n",
    "sample = Image.open(sample_image)\n",
    "sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image shows a visual scene. We need to import the image as an array so the ONNX model we will use can process the image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def transform(image_path, scaled_image_size=640):\n",
    "    image = cv2.imread(image_path)\n",
    "    image, ratio, dwdh = _letterbox_image(image, scaled_image_size, auto=False)\n",
    "    image = image.transpose((2, 0, 1))\n",
    "    image = np.expand_dims(image, 0)\n",
    "    image = np.ascontiguousarray(image)\n",
    "    im = image.astype(np.float32)\n",
    "    im /= 255\n",
    "    return im, ratio, dwdh\n",
    "\n",
    "\n",
    "def _letterbox_image(\n",
    "        im, image_size, color=(114, 114, 114), auto=True, scaleup=True, stride=32):\n",
    "\n",
    "    shape = im.shape[:2]\n",
    "    new_shape = image_size\n",
    "    if isinstance(new_shape, int):\n",
    "        new_shape = (new_shape, new_shape)\n",
    "\n",
    "    r = min(new_shape[0] / shape[0], new_shape[1] / shape[1])\n",
    "    if not scaleup:\n",
    "        r = min(r, 1.0)\n",
    "\n",
    "    new_unpad = int(round(shape[1] * r)), int(round(shape[0] * r))\n",
    "    dw, dh = new_shape[1] - new_unpad[0], new_shape[0] - new_unpad[1]\n",
    "\n",
    "    if auto:\n",
    "        dw, dh = np.mod(dw, stride), np.mod(dh, stride)\n",
    "\n",
    "    dw /= 2\n",
    "    dh /= 2\n",
    "\n",
    "    if shape[::-1] != new_unpad:\n",
    "        im = cv2.resize(im, new_unpad, interpolation=cv2.INTER_LINEAR)\n",
    "    top, bottom = int(round(dh - 0.1)), int(round(dh + 0.1))\n",
    "    left, right = int(round(dw - 0.1)), int(round(dw + 0.1))\n",
    "    im = cv2.copyMakeBorder(\n",
    "        im, top, bottom, left, right, cv2.BORDER_CONSTANT, value=color\n",
    "    )\n",
    "\n",
    "    return im, r, (dw, dh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "converted_image, scale_factor, padding = transform(sample_image)\n",
    "converted_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in a model\n",
    "\n",
    "The model we are going to use today is the Tiny YOLO v3 model, which you can download from the ONNX Model Zoo [here](https://github.com/onnx/models/tree/main/vision/object_detection_segmentation/tiny-yolov3). The model has been trained on the [COCO](https://cocodataset.org/#home) data set, and can recognise 80 types of objects. We begin by loading in the model.\n",
    "\n",
    "The model is stored in an s3 bucket, and we connect to it using the `boto3` library. The `boto3` library was built into the object detection notebook image, which we selected from the spawner page. As such, it is already installed in our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_endpoint_url = environ.get('AWS_S3_ENDPOINT')\n",
    "s3_access_key = environ.get('AWS_ACCESS_KEY_ID')\n",
    "s3_secret_key = environ.get('AWS_SECRET_ACCESS_KEY')\n",
    "s3_bucket_name = environ.get('AWS_S3_BUCKET')\n",
    "\n",
    "print('Imported s3 library')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3 = client(\n",
    "    's3', endpoint_url=s3_endpoint_url,\n",
    "    aws_access_key_id=s3_access_key, aws_secret_access_key=s3_secret_key,\n",
    ")\n",
    "s3.download_file(s3_bucket_name, f'models/{model_id}/1/{model_id}.onnx', 'model.onnx')\n",
    "\n",
    "print('Downloaded model.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should be able to see that this file has been added to your file directory on the left hand side of the screen.\n",
    "\n",
    "Let's now use the model to run object detection on our sample image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "session = InferenceSession('model.onnx')\n",
    "raw_result = session.run(\n",
    "    [], {'images': converted_image}\n",
    ")[0]\n",
    "raw_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postprocess(\n",
    "        prediction,\n",
    "        class_labels,\n",
    "        conf_thres=0.2,\n",
    "        iou_thres=0.6,\n",
    "        max_det=300,\n",
    "        nm=0,\n",
    "):\n",
    "    prediction = torch.Tensor(prediction)\n",
    "    bs = prediction.shape[0]\n",
    "    nc = prediction.shape[2] - nm - 5\n",
    "    xc = prediction[..., 4] > conf_thres\n",
    "\n",
    "    max_wh = 7680\n",
    "    max_nms = 30000\n",
    "\n",
    "    mi = 5 + nc\n",
    "    output = [torch.zeros((0, 6 + nm), device=prediction.device)] * bs\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for xi, x in enumerate(prediction):\n",
    "        x = x[xc[xi]]\n",
    "\n",
    "        if not x.shape[0]:\n",
    "            continue\n",
    "\n",
    "        x[:, 5:] *= x[:, 4:5]\n",
    "        box = _xywh2xyxy(x[:, :4])\n",
    "        mask = x[:, mi:]\n",
    "        conf, j = x[:, 5:mi].max(1, keepdim=True)\n",
    "        x = torch.cat((box, conf, j.float(), mask), 1)[\n",
    "            conf.view(-1) > conf_thres\n",
    "        ]\n",
    "\n",
    "        n = x.shape[0]\n",
    "        if not n:\n",
    "            continue\n",
    "        elif n > max_nms:\n",
    "            x = x[x[:, 4].argsort(descending=True)[:max_nms]]\n",
    "        else:\n",
    "            x = x[x[:, 4].argsort(descending=True)]\n",
    "\n",
    "        c = x[:, 5:6] * max_wh\n",
    "        boxes = x[:, :4] + c\n",
    "        scores = x[:, 4]\n",
    "        i = torchvision.ops.nms(boxes, scores, iou_thres)\n",
    "        if i.shape[0] > max_det:\n",
    "            i = i[:max_det]\n",
    "\n",
    "        output[xi] = x[i]\n",
    "\n",
    "        final_boxes = np.array(output[xi][..., :4])\n",
    "        final_boxes = final_boxes.round().astype(np.int32).tolist()\n",
    "        cls_id = np.array(output[xi][..., 5], dtype=int)\n",
    "        scores = np.array(output[xi][..., 4])\n",
    "        names = [class_labels[id_] for id_ in cls_id]\n",
    "\n",
    "        results.append([final_boxes, scores, names])\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def _xywh2xyxy(x):\n",
    "    y = torch.zeros_like(x) if isinstance(x, torch.Tensor) else np.zeros_like(x)\n",
    "    y[:, 0] = x[:, 0] - x[:, 2] / 2\n",
    "    y[:, 1] = x[:, 1] - x[:, 3] / 2\n",
    "    y[:, 2] = x[:, 0] + x[:, 2] / 2\n",
    "    y[:, 3] = x[:, 1] + x[:, 3] / 2\n",
    "    return y\n",
    "\n",
    "\n",
    "def _box_iou(box1, box2, eps=1e-7):\n",
    "    (a1, a2), (b1, b2) = (\n",
    "        box1.unsqueeze(1).chunk(2, 2),\n",
    "        box2.unsqueeze(0).chunk(2, 2)\n",
    "    )\n",
    "    inter = (torch.min(a2, b2) - torch.max(a1, b1)).clamp(0).prod(2)\n",
    "    return inter / ((a2 - a1).prod(2) + (b2 - b1).prod(2) - inter + eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = postprocess(raw_result, class_labels)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model has returned arrays, each of which holds information about the detected objects. The information includes identifiers for the types of objects, coordinates locating the objects within the image, and detection scores, corresponding to how certain the model is about its prediction.\n",
    "\n",
    "We can use a few functions to help us to superimpose the information in this dictionary onto the original image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_boxes(image_path, boxes, scores, labels, scaling, padding, class_labels):\n",
    "    image = cv2.imread(image_path)  # Read image\n",
    "    colors = {\n",
    "        name: [\n",
    "            random.randint(0, 255) for _ in range(3)\n",
    "        ] for i, name in enumerate(class_labels)\n",
    "    }\n",
    "    for i, (x0, y0, x1, y1) in enumerate(boxes):\n",
    "        box = np.array([x0, y0, x1, y1]).astype('float64')\n",
    "        box -= np.array(padding*2)\n",
    "        box /= scaling\n",
    "        box = box.round().astype(np.int32).tolist()\n",
    "        score = round(float(scores[i]), 3)\n",
    "        name = labels[i]\n",
    "        color = colors[name]\n",
    "        name += ' '+str(score)\n",
    "        cv2.rectangle(image, box[:2], box[2:], color, 2)\n",
    "        cv2.putText(\n",
    "            image,\n",
    "            name,\n",
    "            (box[0], box[1] - 2),\n",
    "            cv2.FONT_HERSHEY_SIMPLEX,\n",
    "            0.75,\n",
    "            [0, 255, 0],\n",
    "            thickness=2\n",
    "        )\n",
    "    img = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(24, 12)\n",
    "    plt.axis('off')\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boxes = result[0][0]\n",
    "scores = result[0][1]\n",
    "labels = result[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "draw_boxes(sample_image, boxes, scores, labels, scale_factor, padding, class_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fantastic! So you've seen how we can use a pre-trained model to identify objects in images. In the next notebooks, we will deploy this model using RHOAI Model Serving, which allows us to use it as part of a larger application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3.9.16",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
